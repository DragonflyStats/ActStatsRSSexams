
\documentclass[a4paper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}

%\usepackage{bigints}
\usepackage{vmargin}

% left top textwidth textheight headheight

% headsep footheight footskip

\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}

\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}

\begin{document}



%%--- Question 8
 Let $\bar{X}_1$ denote the mean of a random sample of size n from a normal population with
mean
2
1 ,
and variance
and let $\bar{X}_2$ denote the mean of a random sample also of
size n from a normal population with the same mean
two samples are independent.
but with variance
2
2 .
The
Define W as the weighted average of the sample means

\[ W =\alpha {\bar{X}_1}  \;+\;(1\;-\;\alpha ){\bar{X}_2} \]


\begin{enumerate}[(i)]
\item (i) Show that $W$ is an unbiased estimator of $\mu$. 
\item (ii) Obtain an expression for the mean square error of $W$. 
\item (iii) Show that the value of
given by
2
2
2
1
2
2
for which W has minimum mean square error is
,
and verify that the optimum corresponds to a minimum.
9
\item 
(iv) Consider the special case when the variances of the two random samples are
equal to a common value $\sigma^2$ . State (do not derive) the maximum likelihood
estimator of calculated from the combined samples, and compare it with the
estimator obtained in (iii).
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%8
(i)
\begin{eqnarray*}
E ( W ) &=& E ( \alpha X 1 + (1 − \alpha ) X 2 )\\
&=& \alpha E ( X 1 ) + (1 − \alpha ) E ( X 2 ) \\ &=& \alpha\mu  + (1 − \alpha ) \mu  \\ &=& \mu \\
\end{eqnarray*}

Therefore W is unbiased.
(ii)
MSE(W) = var(W) + {bias(W)} 2
W is unbiased
\begin{eqnarray*} 
MSE(W) &=& var(W)\\
&=& var( \alpha X 1 + (1 − \alpha ) X 2 )\\
&=& \alpha 2 var( X 1 ) + (1 − \alpha ) 2 var( X 2 ) (independent samples)\\
&=& \alpha 2
\end{eqnarray*}


(iii)
\begin{itemize}
    \item 

\sigma^{2}_{1}
\sigma 2
+ (1 − \alpha ) 2 2
n
n
\sigma 2
\sigma 2
d MSE
= 2 \alpha 1 − 2(1 − \alpha ) 2
d \alpha
n
n
d MSE
= 0 ⇒ ( \sigma^{2}_{1} + \sigma^{2}_{2} ) \alpha = \sigma^{2}_{2}
d \alpha
%%---Page 6Subject CT3 (Probability and Mathematical Statistics Core Technical) — September 2006 — Examiners’ Report
\sigma 22
∴ 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\[ \alpha = \frac{\sigma^2_2}{\sigma^2_1 \;+\; \sigma^2_2} \]

\[  \frac{d^2 \mathrm{MSE}}{d \alpha^2} = 2\frac{\sigma^2_1}{n} + 2 \frac{\sigma^2_2}{n}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

(iv)
= 2
\sigma^{2}_{1}
\sigma 2
+ 2 2 > 0 ∴ minimum
n
n
%---------------------------------------------%
\item The maximum likelihood estimator of $\mu$  in the special case with
$\sigma^{2}_{1} = \sigma^{2}_{2} = \sigma^2$ is

\mu  ˆ =
=
nX + nX 2
sum of observations
= 1
number of observations
2 n
1
1
X 1 + X 2
2
2

\item 
This is the same as $W$ since
\[ \alpha = \frac{\sigma^2_2}{\sigma^2_1 \;+\; \sigma^2_2} 
= \frac{\sigma^2}{\sigma^2 \;+\; \sigma^2}  \;=\; =\frac{1}{2}\]

Therefore

\[ W = \frac{1}{2}  \bar{X}_{1} \;+\; \frac{1}{2}  \bar{X}_{1}\]


\end{itemize}
\end{document}
