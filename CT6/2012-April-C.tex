\documentclass[a4paper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}

%\usepackage{bigints}
\usepackage{vmargin}

% left top textwidth textheight headheight

% headsep footheight footskip

\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}

\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}

\begin{document}

\begin{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%
The total claim amount per annum on a particular insurance policy follows a normal
distribution with unknown mean θ and variance 200 2 . Prior beliefs about θ are
described by a normal distribution with mean 600 and variance 50 2 . Claim amounts
x 1 , x 2 , ... ., x n are observed over n years.
(i) State the posterior distribution of θ .
(ii) Show that the mean of the posterior distribution of θ can be written in the
form of a credibility estimate.
[3]
[2]
Now suppose that n=5 and that total claims over the five years were 3,400.
(iii)
6
Calculate the posterior probability that θ is greater than 600.
[2]
[Total 7]
A proportion p of packets of a rather dull breakfast cereal contain an exciting toy
(independently from packet to packet). An actuary has been persuaded by his
children to begin buying packets of this cereal. His prior beliefs about p before
opening any packets are given by a uniform distribution on the interval [0,1]. It turns
out the first toy is found in the n 1 th packet of cereal.
(i)
Specify the posterior distribution of p after the first toy is found.
[3]
A further toy was found after opening another n 2 packets, another toy after opening
another n 3 packets and so on until the fifth toy was found after opening a grand total
of n 1 + n 2 + n 3 + n 4 + n 5 packets.
(ii) Specify the posterior distribution of p after the fifth toy is found.
(iii) Show the Bayes’ estimate of p under quadratic loss is not the same as the
maximum likelihood estimate and comment on this result.
[5]
[Total 10]
CT6 A2012–3
[2]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

5
(i)
The posterior distribution of θ is Normal with variance given by
σ * 2 =
1
1 ⎞
⎛ n
+ 2 ⎟
⎜
2
50 ⎠
⎝ 200
And mean given by
600 ⎞
⎛ nx
μ * = σ * 2 ⎜
+ 2 ⎟
2
50 ⎠
⎝ 200
(ii)
Set
Z = σ * 2
n
200 2
Then
Z =
n
200 2
1 ⎞
⎛ n
+ 2 ⎟
⎜
2
50 ⎠
⎝ 200
=
n
( n + 16)
And
1 − Z =
1
50 2
1 ⎞
⎛ n
+ 2 ⎟
⎜
2
50 ⎠
⎝ 200
And so
μ * = Zx + ( 1 − Z ) 600
Page 6
= σ * 2
1
50 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55

Which is in the form of a credibility estimate with 600 being the prior mean,
x being the observed sample mean and Z being the credibility factor.
(iii)
In this case we have
σ * 2 =
1
1 ⎞
⎛ n
+ 2 ⎟
⎜
2
50 ⎠
⎝ 200
=
1
1 ⎞
⎛ 5
+ 2 ⎟
⎜
2
50 ⎠
⎝ 200
= 43.64 2
and
600 ⎞
⎛ nx
2 ⎛ 3400 600 ⎞
43.64
μ * = σ * 2 ⎜
+
=
+
⎟
⎜
⎟ = 619.0476
⎝ 200 2 50 2 ⎠
⎝ 200 2 50 2 ⎠
So
( (
)
P ( θ > 600 ) = P N 619.0476, 43.64 2 > 600
)
600 − 619.0476 ⎞
⎛
= P ⎜ N ( 0,1 ) >
⎟ = P ( N ( 0,1 ) > − 0.436)
43.64
⎝
⎠
= 0.6 × 0.67003 + 0.4 × 0.66640
= 0 . 669
This question was well answered. Some candidates attempted to derive the answer to part (i)
from first principles which was not required. Parts (ii) and (iii) were generally answered
well.
Page 7

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
6
(i)
The posterior distribution has a likelihood given by
f ( p n 1 ) ∝ f ( n 1 p ) f ( p )
∝ (1 − p ) n 1 − 1 p × 1
Which is the pdf of a Beta distribution with parameters α = 2 and β = n 1 .
(ii)
Now the posterior distribution has likelihood given by
f ( p n 1 , n 2 , ... , n 5 ) ∝ f ( n 1 , n 2 , ... , n 5 p ) f ( p )
∝ (1 − p ) n 1 − 1 p × (1 − p ) n 2 − 1 p × " × (1 − p ) n 5 − 1 × p
∝ (1 − p ) n 1 + n 2 + " + n 5 − 5 × p 5
Which is the pdf of a Beta distribution with parameters α = 6 and
β = n 1 + n 2 + " + n 5 − 4 .
(iii)
Under squared error loss the Bayes estimate is given by the mean of the
posterior distribution which in this case is
p ˆ =
α
6
=
α + β n 1 + n 2 + " + n 5 + 2
The maximum likelihood estimate is given by maximising the likelihood
which is
L ∝ (1 − p ) n 1 + n 2 + " + n 5 − 5 × p 5
The log-likelihood is given by
l = log L = log C + ( n 1 + " + n 5 − 5 ) log(1 − p ) + 5log p
And so
dl
1
5
= − ( n 1 + " + n 5 − 5 ) ×
+
dp
1 − p p
And setting this expression to zero gives
( n 1 + " + n 5 − 5 ) p ˆ = 5(1 − p ˆ )
And so ( n 1 + " + n 5 ) p ˆ = 5

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
i.e. p̂ =
5
n 1 + " + n 5
So the two estimates are not the same. This is perhaps a little surprising given
that we started with an uninformative prior, but arises because the estimates
are calculated in two different ways – i.e. one maximises the likelihood and
the other minimises the expected squared error. If we wanted the two to be the
same we should use an “all-or-nothing” loss function.
A reasonably well answered question. Weaker candidates failed to identify the geometric
distribution in part (i). Stronger candidates demonstrated a good understanding of loss
functions in part (iii).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
\end{document}
