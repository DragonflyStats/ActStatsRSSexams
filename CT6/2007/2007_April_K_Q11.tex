\documentclass[a4paper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}

%\usepackage{bigints}
\usepackage{vmargin}

% left top textwidth textheight headheight

% headsep footheight footskip

\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}

\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
11
The number, X, of claims on a given insurance policy over one year has probability
distribution given by
\[ P ( X = k ) = \theta  k (1 − \theta  ) k = 0, 1, 2, ...\]
where $\theta$   is an unknown parameter with $0 < \theta  < 1$ .
Independent observations x 1 , ... , x n are available for the number of claims in the previous n years. Prior beliefs about $\theta$   are described by a distribution with density

\[ f ( \theta  ) \propto \theta  \alpha − 1 (1 − \theta  ) \alpha − 1\]
for some constant $\alpha  > 0$ .

\begin{enumerate}
\item (i)
(a) Derive the maximum likelihood estimate, \theta ̂ , of $\theta$   given the data x 1 , ... , x n .
(b) Derive the posterior distribution of $\theta$  given the data x 1 , ... , x n .
(c) Derive the Bayesian estimate of $\theta$   under quadratic loss and show that it takes the form of a credibility estimate
\[Z \theta  ˆ + (1 − Z ) \mu \]
where \mu  is a quantity you should specify from the prior distribution of
$\theta$  .
(d)
Explain what happens to Z as the number of years of observed data
increases.

\item (ii)
(a) Determine the variance of the prior distribution of $\theta$  .
(b) Explain the implication for the quality of prior information of increasing the value of \alpha . Give an interpretation of the prior distribution in the special case \alpha  = 1.

\item (iii)
Calculate the Bayesian estimate of $\theta$   under quadratic loss if n = 3,
x 1 = 3, x 2 = 3, x 3 = 5 and
(a)
(b)
\alpha  = 5
\alpha  = 2
Comment on your results in the light of (ii) above.
\end{enumerate}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5555
11
(i)
(a)
The likelihood is given by
\theta  x n (1 − \theta  )
L \propto \theta  x 1 (1 − \theta  )
= \theta  x 1 +
x n
(1 − \theta  ) n
and so the log-likelihood is given by
l = LogL = ( x 1 +
+ x n ) log \theta  + n log(1 − \theta  )
and differentiating gives
dl x 1 + + x n
n
=
−
1 − \theta 
d \theta 
\theta 
setting this expression to zero to find the maximum gives
x 1 + + x n
( x 1 + + x n ) = ( n + x 1 +
n
= 0
1 − \theta  ˆ
\theta  ˆ
( x 1 + + x n )(1 − \theta  ˆ ) = n \theta  ˆ
\theta  ˆ =
−
+ x n ) \theta  ˆ
x 1 + + x n
n + x 1 + + x n
to check this is a maximum, note that
d 2 l
d \theta 
(b)
2
=−
x 1 +
+ x n
\theta 
2
−
n
(1 − \theta  ) 2
< 0
The posterior distribution is given by
f ( \theta  x ) \propto g ( x \theta  ) f ( \theta  )
= \theta  x 1 +
+ x n
= \theta  \alpha + x 1 +
(1 − \theta  ) n × \theta  \alpha − 1 (1 − \theta  ) \alpha − 1
+ x n − 1
(1 − \theta  ) n +\alpha − 1
which is the pdf of a beta distribution with parameters \alpha  + x 1 +
and \alpha  + n .
(c)
First note that the prior distribution of $\theta$   is Beta with parameters \alpha 
and \alpha  . Hence its mean is
\alpha 
= 1/ 2 .
\alpha +\alpha 
Page 12
+ x n
%%%%%%%%%%%%%%%%%%%%%%%%
Under Bayesian loss, the estimator is given by the mean of the posterior distribution, which is
\theta  * =
\alpha  + \sum  x i
2 \alpha  + \sum  x i + n
=
2 \alpha 
1
\sum  x i × \sum  x i + n +
×
\sum  x i + n 2 \alpha  + \sum  x i + n 2 \alpha  + \sum  x i + n 2
ˆ Z + (1 − Z ) \mu 
= \theta ×
Where Z =
(ii)
\sum  x i + n
2 \alpha  + \sum  x i + n
and \mu  = 1/ 2 is the prior mean of $\theta$  .
(d) As n increases, Z tends towards 1, and the Bayes estimate approaches the maximum likelihood estimate, as more credibility is put on the data, and less on the prior estimate.
(a) The variance of the prior distribution is given by:
\alpha  2
2
(2 \alpha  ) (2 \alpha  + 1)
(b)
=
1
.
4(2 \alpha  + 1)
Higher values of \alpha  result in a lower variance and hence imply greater certainty over the prior value of $\theta$   .
In the special case where \alpha  = 1 the prior distribution is Uniform on [0,1] implying that we have no particular reason to believe that any prior value of $\theta$  is more or less likely than any other.
(iii)
\sum  x i = 3 + 3 + 5 = 11
(a) \theta  * = 5 + 11
16 2
=
= = 0.6667
2 × 5 + 11 + 3 24 3
(b) \theta  * = 2 + 11
13
= = 0.7222
2 × 2 + 11 + 3 18
The first set of parameters has greater certainty attached to the prior estimate (i.e. a higher value of \alpha  ), and therefore the posterior estimate is closer to the mean of the prior distribution (which is 0.5) than in the second case.
\end{document}
