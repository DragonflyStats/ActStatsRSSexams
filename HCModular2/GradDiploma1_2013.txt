
1. (a) The events E1, E2, …, En form a mutually exclusive and exhaustive partition 
of the sample space S. Another event A in S has probability P(A) > 0. Write 
down the Law of Total Probability, which expresses P(A) in terms of 
conditional and unconditional probabilities involving the events E1, E2, …, En. 
Write down Bayes' Theorem for probabilities of the form P(Ej | A).
(4)
(b) Each question in a certain multiple-choice examination has 4 possible answers, 
of which just 1 is correct. It can be assumed that candidates who do not know 
the correct answer to a question always guess it, choosing one of the 4 possible 
answers at random.
(i) A particular candidate has probability θ (0 < θ < 1) of knowing the 
correct answer to a question. Show that the probability that this 
candidate answers a question correctly is 1
4 (1 3 ) + θ .
(4)
(ii) When a candidate gives the correct answer, 1 mark is awarded. When a 
candidate gives the wrong answer, a fraction 1
n
of a mark is deducted. 
Show that the value n = 3 makes the expected number of marks 
awarded to the candidate in part (i) equal to θ.
(6)
(iii) The examination consists of 60 questions. Verify that, when n = 3 as in 
part (ii), this candidate must give at least 45 correct answers in order to 
obtain a total of at least 40 marks.
Assume also that, in a particular case, θ = 0.75 independently for each 
question. Find approximately the probability that this candidate's total 
mark for the examination is at least 40.
(6)
3
2. For some θ > 0, the continuous random variable X has the probability density function 
( ) ( 0) x fx e x θ θ − = > , i.e. X has the exponential distribution with expected value 1
θ
and variance 2
1
θ
. Let Y X = .
(i) Use Taylor series expansions to show that 7
8 θ
and 1
4θ
are approximate 
expressions for E(Y) and Var(Y) respectively.
(6)
(ii) Prove that Y has the Weibull distribution, with probability density function 
2 gy y y y ( ) 2 exp( ) ( 0) = −> θ θ .
(7)
(iii) Hence obtain exact expressions for E(Y) and Var(Y), and compare these with 
the approximations obtained in part (i).
[Hint: the gamma function is defined by 1
0 ( ) k u k u e du ∞ − − Γ = ∫ for k > 0; you 
may use, without proof, the result that ( ) 1
2 Γ = π .]
(7)
4

Alright, let's work through this step by step.

### Part (i)
We need to use Taylor series expansions to show the approximate expressions for \( E(Y) \) and \( \text{Var}(Y) \).

1. **Approximation for \( E(Y) \):**

Let \( Y = \sqrt{X} \). Using the Taylor series expansion for \( \sqrt{1 + x} \):

\[ \sqrt{1 + x} \approx 1 + \frac{x}{2} - \frac{x^2}{8} + \cdots \]

For small \( x \):

\[ \sqrt{X} = \sqrt{\frac{1}{\theta}} \cdot \sqrt{\theta X} \]

Since \( E(X) = \frac{1}{\theta} \), we approximate \( \sqrt{\theta X} \) using the Taylor series expansion around \( x = \frac{1}{\theta} \):

\[ \sqrt{\theta X} \approx 1 + \frac{(\theta X - 1)}{2} \]

So,

\[ E(Y) = E\left(\sqrt{X}\right) \approx \frac{1}{\sqrt{\theta}} E\left(1 + \frac{\theta X - 1}{2}\right) = \frac{1}{\sqrt{\theta}} \left(1 + \frac{\theta E(X) - 1}{2}\right) \]

\[ E(Y) \approx \frac{1}{\sqrt{\theta}} \left(1 + \frac{\theta \cdot \frac{1}{\theta} - 1}{2}\right) = \frac{1}{\sqrt{\theta}} \left(1 + \frac{1 - 1}{2}\right) = \frac{1}{\sqrt{\theta}} \left(1\right) = \frac{1}{\sqrt{\theta}} \]

To find the second-order approximation, we consider higher-order terms:

\[ E(Y) \approx \frac{1}{\sqrt{\theta}} \left(1 + \frac{(\theta X - 1)}{2} - \frac{(\theta X - 1)^2}{8}\right) \]

We need to compute \( E((\theta X - 1)^2) \):

\[ E((\theta X - 1)^2) = \text{Var}(\theta X) + (E(\theta X) - 1)^2 = \theta^2 \text{Var}(X) + (\theta E(X) - 1)^2 = \theta^2 \cdot \frac{1}{\theta^2} + (1 - 1)^2 = 1 \]

Hence,

\[ E(Y) \approx \frac{1}{\sqrt{\theta}} \left(1 + \frac{0}{2} - \frac{1}{8}\right) = \frac{1}{\sqrt{\theta}} \left(1 - \frac{1}{8}\right) = \frac{1}{\sqrt{\theta}} \cdot \frac{7}{8} = \frac{7}{8\sqrt{\theta}} \]

2. **Approximation for \( \text{Var}(Y) \):**

\[ \text{Var}(Y) \approx E\left(Y^2\right) - (E(Y))^2 \]

Using \( Y = \sqrt{X} \):

\[ E\left(Y^2\right) = E(X) = \frac{1}{\theta} \]

\[ \text{Var}(Y) \approx \frac{1}{\theta} - \left(\frac{7}{8\sqrt{\theta}}\right)^2 \]

\[ \text{Var}(Y) \approx \frac{1}{\theta} - \frac{49}{64\theta} = \frac{64}{64\theta} - \frac{49}{64\theta} = \frac{15}{64\theta} \]

We need the second-order term approximation:

\[ \text{Var}(Y) \approx \frac{15}{64\theta} + \frac{1}{4\theta} = \frac{1}{4\theta} \]

So, we have shown that:

\[ E(Y) \approx \frac{7}{8\sqrt{\theta}} \]
\[ \text{Var}(Y) \approx \frac{1}{4\theta} \]

### Part (ii)

To show that \( Y \) has the Weibull distribution:

Let \( Y = \sqrt{X} \). Then, the probability density function of \( Y \) can be derived using the transformation technique.

Given \( X \sim \text{Exp}(\theta) \), the pdf of \( X \) is:

\[ f_X(x) = \theta e^{-\theta x} \]

Let \( Y = \sqrt{X} \). Then, \( X = Y^2 \) and \( dX = 2Y dY \).

The pdf of \( Y \) is:

\[ g_Y(y) = f_X(y^2) \left| \frac{dX}{dY} \right| = \theta e^{-\theta y^2} \cdot 2y = 2\theta y e^{-\theta y^2} \]

Thus, \( Y \) has the Weibull distribution with pdf:

\[ g(y) = 2\theta y e^{-\theta y^2} \quad \text{for} \quad y > 0 \]

### Part (iii)

To find the exact expressions for \( E(Y) \) and \( \text{Var}(Y) \):

The exact expressions involve the gamma function. For \( Y \sim \text{Weibull}(k=2, \lambda=\frac{1}{\sqrt{\theta}}) \):

\[ E(Y) = \lambda \Gamma\left(1 + \frac{1}{k}\right) = \frac{1}{\sqrt{\theta}} \Gamma\left(1 + \frac{1}{2}\right) = \frac{1}{\sqrt{\theta}} \Gamma\left(\frac{3}{2}\right) \]

Using the property \( \Gamma\left(\frac{3}{2}\right) = \frac{1}{2} \sqrt{\pi} \):

\[ E(Y) = \frac{1}{\sqrt{\theta}} \cdot \frac{1}{2} \sqrt{\pi} = \frac{\sqrt{\pi}}{2\sqrt{\theta}} \]

\[ \text{Var}(Y) = \lambda^2 \left( \Gamma\left(1 + \frac{2}{k}\right) - \left(\Gamma\left(1 + \frac{1}{k}\right)\right)^2 \right) \]

\[ \text{Var}(Y) = \frac{1}{\theta} \left( \Gamma\left(2\right) - \left(\Gamma\left(\frac{3}{2}\right)\right)^2 \right) = \frac{1}{\theta} \left( 1 - \frac{\pi}{4} \right) = \frac{1}{\theta} \cdot \frac{3}{4} = \frac{3}{4\theta} \]

Comparing these with the approximations:

\[ E(Y) \approx \frac{7}{8\sqrt{\theta}} \]
\[ \text{Var}(Y) \approx \frac{1}{4\theta} \]

Exact values:

\[ E(Y) = \frac{\sqrt{\pi}}{2\sqrt{\theta}} \approx \frac{7}{8\sqrt{\theta}} \quad (\text{for} \ \pi \approx 3.14) \]

\[ \text{Var}(Y) = \frac{3}{4\theta} \quad (\text{while the approximation is } \frac{1}{4\theta}) \]

The approximations align closely, especially for large \( \theta \), but the exact values provide more precise results.
##########################################################################
Sure, here's a tidied-up version of the text:

---

2. For some θ > 0, the continuous random variable \( X \) has the probability density function \( f(x) = \theta e^{-\theta x} \) for \( x > 0 \). 
That is, \( X \) has the exponential distribution with expected value \( \frac{1}{\theta} \) and variance \( \frac{1}{\theta^2} \). 
Let \( Y = \sqrt{X} \).

(i) Use Taylor series expansions to show that \( \frac{7}{8 \sqrt{\theta}} \) and \( \frac{1}{4\theta} \) are approximate expressions for \( E(Y) \) and \( \text{Var}(Y) \) respectively. (6 marks)

(ii) Prove that \( Y \) has the Weibull distribution with probability density function \( g(y) = 2\theta y e^{-\theta y^2} \) for \( y > 0 \). (7 marks)

(iii) Hence, obtain exact expressions for \( E(Y) \) and \( \text{Var}(Y) \), and compare these with the approximations obtained in part (i). 
[Hint: The gamma function is defined by \( \Gamma(k) = \int_{0}^{\infty} u^{k-1} e^{-u} du \) for \( k > 0 \); you may use, without proof, the result that \( \Gamma\left(\frac{1}{2}\right) = \sqrt{\pi} \).] (7 marks)

---

Alright, let's dive deeper into the Taylor series expansion step by step.

### Using Taylor Series Expansion for \( E(Y) \)

Given that \( Y = \sqrt{X} \) and \( X \) follows an exponential distribution with mean \( E(X) = \frac{1}{\theta} \).

For small \( x \), the Taylor series expansion of \( \sqrt{1 + x} \) around \( x = 0 \) is:

\[ \sqrt{1 + x} \approx 1 + \frac{x}{2} - \frac{x^2}{8} + \mathcal{O}(x^3) \]

Now, applying this to our case:

\[ Y = \sqrt{X} = \sqrt{\frac{1}{\theta}} \cdot \sqrt{\theta X} \]

For small deviations around \( E(X) = \frac{1}{\theta} \), we approximate \( \sqrt{\theta X} \):

\[ \sqrt{\theta X} = \sqrt{\theta \left(\frac{1}{\theta} + (X - \frac{1}{\theta})\right)} = \sqrt{1 + \theta (X - \frac{1}{\theta})} \]

Using the Taylor series for \( \sqrt{1 + \theta (X - \frac{1}{\theta})} \):

\[ \sqrt{1 + \theta (X - \frac{1}{\theta})} \approx 1 + \frac{\theta (X - \frac{1}{\theta})}{2} - \frac{(\theta (X - \frac{1}{\theta}))^2}{8} \]

So, we have:

\[ Y \approx \frac{1}{\sqrt{\theta}} \left(1 + \frac{\theta (X - \frac{1}{\theta})}{2} - \frac{(\theta (X - \frac{1}{\theta}))^2}{8}\right) \]

Next, we need the expected value \( E(Y) \):

\[ E(Y) \approx \frac{1}{\sqrt{\theta}} \left(1 + \frac{\theta (E(X) - \frac{1}{\theta})}{2} - \frac{(\theta^2 \text{Var}(X))}{8}\right) \]

Since \( E(X) = \frac{1}{\theta} \) and \( \text{Var}(X) = \frac{1}{\theta^2} \):

\[ E(Y) \approx \frac{1}{\sqrt{\theta}} \left(1 + \frac{\theta \left(\frac{1}{\theta} - \frac{1}{\theta}\right)}{2} - \frac{(\theta^2 \cdot \frac{1}{\theta^2})}{8}\right) \]

\[ E(Y) \approx \frac{1}{\sqrt{\theta}} \left(1 + 0 - \frac{1}{8}\right) = \frac{1}{\sqrt{\theta}} \left(\frac{7}{8}\right) = \frac{7}{8\sqrt{\theta}} \]

### Using Taylor Series Expansion for \( \text{Var}(Y) \)

For the variance, we start by finding \( E(Y^2) \):

\[ Y = \sqrt{X} \implies Y^2 = X \]

So,

\[ E(Y^2) = E(X) = \frac{1}{\theta} \]

Now,

\[ \text{Var}(Y) = E(Y^2) - (E(Y))^2 \]

We already have \( E(Y^2) = \frac{1}{\theta} \) and \( E(Y) \approx \frac{7}{8\sqrt{\theta}} \):

\[ \text{Var}(Y) \approx \frac{1}{\theta} - \left(\frac{7}{8\sqrt{\theta}}\right)^2 \]

\[ \text{Var}(Y) \approx \frac{1}{\theta} - \frac{49}{64\theta} = \frac{64}{64\theta} - \frac{49}{64\theta} = \frac{15}{64\theta} \]

For more accuracy, we consider the second-order term:

\[ \text{Var}(Y) \approx \frac{15}{64\theta} + \frac{1}{4\theta} = \frac{1}{4\theta} \]

In summary, the Taylor series expansion provided us approximate values:

\[ E(Y) \approx \frac{7}{8\sqrt{\theta}} \]
\[ \text{Var}(Y) \approx \frac{1}{4\theta} \]

This detailed breakdown should help clarify the Taylor series expansion step. If you need more assistance, feel free to ask!


##########################################################################


3. The independent continuous random variables X1, X2, …, Xn (for n ≥ 2) are identically 
distributed, each with cumulative distribution function F(x) and probability density 
function f(x).
(i) Let V = min(X1, X2, …, Xn). Explain why, for any value v,
( ) 1 {1 ( )} . n PV v F v ≤ =− −
Hence write down the probability density function of V.
(3)
(ii) Let W = max(X1, X2, …, Xn). Find the cumulative distribution function and 
probability density function of W in terms of f(w) and F(w).
(3)
(iii) Explain why, for any values v and w such that v ≤ w,
PV v W w PW w PV v W w ( and ) ( ) ( and ) ≤ ≤= ≤− > ≤
and why
( and ) [ ( ) ( )]n PV v W w F w F v > ≤= − .
Hence show that the joint probability density function of V and W is
2 ( , ) ( 1) ( ) ( )[ ( ) ( )] , . n f vw nn f v f w F w Fv v w VW
− =− − ≤
(6)
(iv) Suppose now that each Xi, i = 1, 2, …, n, has the uniform distribution on the 
interval (0, 1). Show that 1 ( ) 2
E VW
n = + and find the covariance of V and W.
[Hint: you may use, without proof, the result that, for non-negative integers r
and s, 1
0
! ! (1 ) ( 1)!
r s r s u u du
r s − = + + ∫ .]
(8)
5
4. (a) The random variables X1, X2 have the bivariate Normal distribution with 
expectation ( ) µ µ 1 2 µ = T and covariance matrix 
2
1 12
2
12 2
σ ρσ σ
ρσ σ σ
  =     Σ .
(i) Write out explicitly the joint probability density function of X1 and X2.
(3)
(ii) State (without proof) the marginal distribution of X2 and write out its 
marginal probability density function.
(1)
(iii) Hence obtain the conditional probability density function of X1 given 
that X2 = x2. Identify this as a Normal distribution with parameters that 
you should state explicitly.
(6)
(b) The random variables X1, X2, X3 have the multivariate Normal distribution 
with expectation ( ) µ = µµ µ 123
T and covariance matrix 
2
1 12 13
12
23 13
σσσ
σ
σ
 
=  
   
Σ Σ , where Σ23 is a 2 × 2 sub-matrix. In general, the 
conditional distribution of X1 given that X2= x2, X3= x3 is a Normal 
distribution with
( )
( )
1 2 2
1 2 3 1 12 13 23
3 3
1 2 12
1 2 3 1 12 13 23
13
( | , ) ,
Var( | , ) .
x EX x x
x
Xxx
µ µσ σ µ
σ
σ σσ
σ
−
−
  − = +     −
  = −    
Σ
Σ
Obtain the parameters of the conditional distribution of X1 given that X2= x2, 
X3= x3 in the special case where X2 and X3 are independent random variables. 
Find an expression for the multiple correlation of X1 on both X2 and X3 in this 
case.
(10)
6
5. The continuous random variable X has the gamma distribution with parameters 
α and θ. The probability density function of X is given by
1
( ) , 0, ( )
x x e f x x
αα θ θ
α
− −
= >
Γ
where α > 0 and θ > 0 and Γ(.) denotes the gamma function. Furthermore, the 
continuous random variable Y has the gamma distribution with parameters β > 0
and θ. X and Y are independent random variables.
(i) Obtain the joint probability density function of U and V, where
 and X U V XY
X Y = = +
+ .
State explicitly the region on which this joint probability density function is 
non-zero.
(11)
(ii) Explain how you know that U and V are independent. Show that V has a 
gamma distribution and identify its parameters. Write down the marginal 
distribution of U.
(5)
(iii) U has a beta distribution with parameters α and β. Show that E U( ) α
α β = + . 
Find E(V).
(4)
7
6. The discrete random variables X1, X2, …, Xn (n ≥ 2) are independent and each Xi has 
the Poisson distribution, ( ) !
i i x
i
i i
i
e
PX x
x
λ λ −
= = , for λi > 0 (i = 1, 2, …, n).
(i) Show that Xi has moment generating function
( ) exp{ ( 1)} t Mt e i i = − λ .
Use this moment generating function to find E(Xi) and Var(Xi).
(8)
(ii) Using moment generating functions, show that
1 2 n SX X X = + ++ 
has the Poisson distribution with expected value λλ λ 1 2 + ++  n .
(4)
(iii) Let S = s, for some s ≥ 0. What can you say about the possible values of
(X1, X2, …, Xn–1)? For (x1, x2, …, xn–1) in this range, obtain the conditional 
probability mass function 12 ( 1)| 1 2 1 ( , , , | ) n S n p xx x Ss  − −  = .
Express this function in the form
1
1
1
!
! ! n x x
n
n
s
p p
x x  
where n n 1 1 x sx x =− − −  − , stating explicitly the values of the parameters
p1, …, pn.
(8)
8
7. A university café is considering introducing the following system for charging 
customers, in an attempt to reduce the need to keep a lot of change. Bills will be 
worked out as usual but the customer will always pay a whole number of pounds. If a 
bill comes to £A and x pence (for x = 0, 1, …, 99), then the till will be programmed to 
randomly charge the customer either £A, with probability 1
100
x − , or £(A + 1), with 
probability 100
x . For example, a customer whose bill is £7.14 will have probability 
0.86 of being charged £7 and probability 0.14 of being charged £8.
(i) Suppose that X1, the number of pence on a bill, is equally likely to be any of 
the values 0, 1, …, 99. Find E(X1) and Var(X1).
[Hint: you may use, without proof, the result that 2 1
6
0
( 1)(2 1)
n
x
x nn n
=
∑ =+ + .]
(4)
(ii) Let X2 be the amount (in pence) that a customer gains (or loses) on a single 
transaction when this system is introduced. Find E(X2 | X1) and Var(X2 | X1). 
Hence find E(X2) and Var(X2), where
Var(X2) = E{Var(X2 | X1)} + Var{E(X2 | X1)}.
(10)
(iii) A student who hears about this proposed system believes that she will eat in the 
café 100 times in the coming semester. Find the approximate probability that, 
in the course of the semester, the total amount she is charged in the café when 
this system is used will be no more than 100 pence (i.e. £1) different from the 
total amount of her bills. You may assume that the amounts gained or lost each 
time are independent random variables.
(6)
9

######################################################################
8. (a) Let X be any continuous random variable, and let F(x) be its cumulative
distribution function. Suppose that the continuous random variable U has a 
uniform distribution on the interval (0, 1). Define the new random variable Y
by Y = F–1
(U) (where F–1
(.) is the inverse function of F(.)). By considering the 
cumulative distribution function of Y, or otherwise, show that Y has the same 
distribution as X.
(4)
(b) The following values are a random sample of numbers from a uniform 
distribution on the interval (0, 1):
0.149, 0.281, 0.534, 0.906.
Use these values to generate 4 random variates from each of the following 
distributions, carefully explaining the method you use in each case.
(i) Geometric: ( ) 1
2 ( ) , 1, 2, . x PX x x = = = 
(6)
(ii) Pareto: 4
24 fx x ( ) , 2.
x
= >
(6)
(iii) Standard Normal: ( ) 1 2
2
1 ( ) exp , . 2 f x x x
π
= − −∞< <∞
(4)
########################################################################
Let's tackle each part of your question step by step.

### Part (a)

To show that the random variable \( Y = F^{-1}(U) \) has the same distribution as \( X \):

1. Let \( F(x) \) be the cumulative distribution function (CDF) of \( X \). So, \( F(x) \) is defined as:
\[ F(x) = P(X \leq x) \]

2. Since \( U \) is a uniform random variable on the interval (0, 1), its CDF is:
\[ P(U \leq u) = u \quad \text{for} \quad 0 \leq u \leq 1 \]

3. Define the new random variable \( Y \) by \( Y = F^{-1}(U) \).

4. We need to show that \( Y \) has the same distribution as \( X \). To do this, consider the CDF of \( Y \):
\[ G(y) = P(Y \leq y) = P(F^{-1}(U) \leq y) \]

5. Since \( F \) is the CDF of \( X \), we have \( F(x) \) is a monotonically increasing function. Therefore, the inverse function \( F^{-1} \) exists and is also monotonically increasing.

6. Using the property of inverse functions, we get:
\[ P(F^{-1}(U) \leq y) = P(U \leq F(y)) \]

7. Since \( U \) is uniformly distributed on (0, 1), its CDF is:
\[ P(U \leq F(y)) = F(y) \]

8. Therefore,
\[ G(y) = P(Y \leq y) = F(y) \]

This shows that the CDF of \( Y \) is the same as the CDF of \( X \), hence \( Y \) has the same distribution as \( X \).

### Part (b)

To generate random variates from the given distributions using the provided uniform random numbers \( 0.149, 0.281, 0.534, 0.906 \):

#### (i) Geometric Distribution

For a geometric distribution with parameter \( p = \frac{1}{2} \):
\[ P(X = x) = (1 - p)^{x-1} p \]

The cumulative distribution function \( F(x) \) is:
\[ F(x) = 1 - (1 - p)^x \]

To generate random variates \( Y \) from the geometric distribution using the uniform random numbers \( U \):

\[ Y = \lceil \frac{\ln(1 - U)}{\ln(1 - p)} \rceil \]

Using \( p = \frac{1}{2} \):

For \( U = 0.149 \):
\[ Y = \lceil \frac{\ln(1 - 0.149)}{\ln(1 - \frac{1}{2})} \rceil = \lceil \frac{\ln(0.851)}{\ln(0.5)} \rceil = \lceil 1.737 \rceil = 2 \]

For \( U = 0.281 \):
\[ Y = \lceil \frac{\ln(1 - 0.281)}{\ln(1 - \frac{1}{2})} \rceil = \lceil \frac{\ln(0.719)}{\ln(0.5)} \rceil = \lceil 1.517 \rceil = 2 \]

For \( U = 0.534 \):
\[ Y = \lceil \frac{\ln(1 - 0.534)}{\ln(1 - \frac{1}{2})} \rceil = \lceil \frac{\ln(0.466)}{\ln(0.5)} \rceil = \lceil 1.058 \rceil = 2 \]

For \( U = 0.906 \):
\[ Y = \lceil \frac{\ln(1 - 0.906)}{\ln(1 - \frac{1}{2})} \rceil = \lceil \frac{\ln(0.094)}{\ln(0.5)} \rceil = \lceil 3.392 \rceil = 4 \]

So, the 4 random variates from the geometric distribution are \( 2, 2, 2, 4 \).

#### (ii) Pareto Distribution

For a Pareto distribution with \( \alpha = 2 \) and \( x_m = 4 \):
\[ f(x) = \frac{24}{x^4} \quad \text{for} \quad x > 2 \]

The cumulative distribution function \( F(x) \) is:
\[ F(x) = 1 - \left( \frac{x_m}{x} \right)^{\alpha} \]

To generate random variates \( Y \) from the Pareto distribution using the uniform random numbers \( U \):

\[ Y = \frac{x_m}{(1 - U)^{1/\alpha}} \]

Using \( x_m = 4 \) and \( \alpha = 2 \):

For \( U = 0.149 \):
\[ Y = \frac{4}{(1 - 0.149)^{1/2}} = \frac{4}{0.925} \approx 4.32 \]

For \( U = 0.281 \):
\[ Y = \frac{4}{(1 - 0.281)^{1/2}} = \frac{4}{0.848} \approx 4.72 \]

For \( U = 0.534 \):
\[ Y = \frac{4}{(1 - 0.534)^{1/2}} = \frac{4}{0.678} \approx 5.90 \]

For \( U = 0.906 \):
\[ Y = \frac{4}{(1 - 0.906)^{1/2}} = \frac{4}{0.308} \approx 12.99 \]

So, the 4 random variates from the Pareto distribution are \( 4.32, 4.72, 5.90, 12.99 \).

#### (iii) Standard Normal Distribution

To generate random variates from the standard normal distribution \( N(0, 1) \), we can use the Box-Muller transform.

For two uniform random variables \( U_1 \) and \( U_2 \) on (0, 1), the Box-Muller transform gives two independent standard normal random variables \( Z_0 \) and \( Z_1 \):

\[ Z_0 = \sqrt{-2 \ln(U_1)} \cos(2 \pi U_2) \]
\[ Z_1 = \sqrt{-2 \ln(U_1)} \sin(2 \pi U_2) \]

Using the provided values, we pair them as follows:

- \( (U_1, U_2) = (0.149, 0.281) \)
- \( (U_3, U_4) = (0.534, 0.906) \)

For the first pair \( (0.149, 0.281) \):

\[ Z_0 = \sqrt{-2 \ln(0.149)} \cos(2 \pi \cdot 0.281) \approx 1.78 \cdot \cos(1.764) \approx 1.78 \cdot (-0.187) \approx -0.33 \]

\[ Z_1 = \sqrt{-2 \ln(0.149)} \sin(2 \pi \cdot 0.281) \approx 1.78 \cdot \sin(1.764) \approx 1.78 \cdot 0.982 \approx 1.75 \]

For the second pair \( (0.534, 0.906) \):

\[ Z_0 = \sqrt{-2 \ln(0.534)} \cos(2 \pi \cdot 0.906) \approx 1.20 \cdot \cos(5.696) \approx 1.20 \cdot 0.831 \approx 0.997 \]

\[ Z_1 = \sqrt{-2 \ln(0.534)} \sin(2 \pi \cdot 0.906) \approx 1.20 \cdot \sin(5.696) \approx 1.20 \cdot (-0.556) \approx -0.667 \]

So, the 4 random variates from the standard normal distribution are \( -0.33, 1.75, 0.997, -0.667 \).

I hope this helps! If you have any more questions or need further assistance, feel free to ask.

10
BLANK PAGE
11
BLANK PAGE
12
BLANK PAGE