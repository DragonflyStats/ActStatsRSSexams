1 HC Module 5 2012
This examination paper consists of 4 printed pages, each printed on one side only.
This front cover is page 1.
Question 1 starts on page 2.
There are 4 questions altogether in the paper.
© RSS 2012
EXAMINATIONS OF THE HONG KONG STATISTICAL SOCIETY
HIGHER CERTIFICATE IN STATISTICS, 2012
MODULE 5 : Further probability and inference
Time allowed: One and a half hours
Candidates should answer THREE questions.
Each question carries 20 marks.
The number of marks allotted for each part-question is shown in brackets.
Graph paper and Official tables are provided.
Candidates may use calculators in accordance with the regulations published in
the Society's "Guide to Examinations" (document Ex1).
The notation log denotes logarithm to base e.
Logarithms to any other base are explicitly identified, e.g. log10.
Note also that
n
r
is the same as n
r C .
2
Turn over
1. The continuous random variables X and Y are jointly distributed with joint probability
density function
(1 ) 0, 0,
( , )
0 otherwise,
y x xe x y
f x y
and the discrete random variables W and V are defined by
0 if 1, 0 if 1,
1 if 1, 1 if 1.
X Y
W V
X Y
(i) Find the marginal probability density functions of X and Y.
(6)
(ii) Find the conditional density function f (y | x) for x > 0 and hence evaluate
E(Y | X = x).
(5)
(iii) Find P(W = 1), P(V = 1) and P(W = V = 1).
(5)
(iv) Find Cov(W, V).
(4)
2. (a) Suppose that X is a discrete random variable which can only take non-negative
integer values (i.e. 0, 1, 2, …). Define the probability generating function X(t)
of X.
(2)
(b) The discrete random variable Y has probability distribution
1
( ) for 1, 2, 3, ,
( 1)!
k e
P Y k k
k
where is a positive parameter.
(i) Show that the probability generating function of Y is ( t 1) te .
(4)
(ii) Use the probability generating function of Y to evaluate the mean and
variance of Y. Outline how you would find E(Y
3
).
(8)
(iii) Suppose that Y1, Y2, …, Yn are independent random variables, each with
the same distribution as Y, and that
1
n
i
i
W Y. Find the probability
generating function of W and hence find P(W = k) for k = 0, 1, 2, 3, … .
(6)
3
Turn over
3. (a) What is meant by a maximum likelihood estimator? What good properties do
maximum likelihood estimators possess, assuming that the necessary regularity
conditions hold?
(8)
(b) The probability of obtaining heads when a coin is tossed is p, where 0 < p < 1
and p is unknown. A game consists of two independent tosses of the coin, with
the player winning if there is exactly one head. In 100 independent games, the
player wins 30 times.
(i) Show that the likelihood of p is proportional to
30 30 70 p (1 p) (1 2p(1 p))
for 0 < p < 1.
(5)
(ii) The plot of the logarithm of the likelihood shown below indicates that
there are two values of p that give the same maximum value for the
likelihood. Explain why this happens and find an equation satisfied by
these values. (Do not attempt to solve this equation.)
(7)
4
4. Random variables X1 and X2 are independent, each having a Normal distribution with
mean and variance both equal to > 0, an unknown parameter. It is required to use
X1 and X2 to estimate .
(i) Find the method of moments estimator ˆ of based on the first sample
moment. Show that ˆ is unbiased and find its variance.
(4)
(ii) The random variable Y = X1 – X2. State the distribution of Y.
(1)
(iii) Show that ˆ and Y are uncorrelated.
(3)
(iv) Another unbiased estimator of is 2 kY , where k is a constant. Find the
value of k.
(4)
(v) Show that E(Y
4) = 12
2
.
(4)
[You may use the result that the moment generating function of a Normal
distribution with mean and variance
2 is 1 2 2
2 exp( t t ) .]
(vi) Use part (v) to find the variance of , and hence the efficiency of ˆ relative
to .
(4)
