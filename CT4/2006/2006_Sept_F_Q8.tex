\documentclass[a4paper,12pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{chngpage}

%\usepackage{bigints}
\usepackage{vmargin}

% left top textwidth textheight headheight

% headsep footheight footskip

\setmargins{2.0cm}{2.5cm}{16 cm}{22cm}{0.5cm}{0cm}{1cm}{1cm}

\renewcommand{\baselinestretch}{1.3}

\setcounter{MaxMatrixCols}{10}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%- 8
Let X 1 denote the mean of a random sample of size $n$ from a normal population with
mean
2
1 ,
and variance
and let $X_2$ denote the mean of a random sample also of size n from a normal population with the same mean
two samples are independent.
but with variance
2
2 .
The
Define W as the weighted average of the sample means
W
X 1 (1
) X 2

\begin{enumerate}[(a)]
\item Show that W is an unbiased estimator of . 
\item Obtain an expression for the mean square error of W.
\item  Show that the value of
given by
2
2
2
1
2
2
for which W has minimum mean square error is
,
and verify that the optimum corresponds to a minimum.
9
\item  Consider the special case when the variances of the two random samples are equal to a common value 2 . State (do not derive) the maximum likelihood estimator of calculated from the combined samples, and compare it with the estimator obtained in \item (iii).
\end{enumerate}
\newoage
8
\begin{itemize}
\item (i)
\begin{eqnarray*}
E ( W ) 
&=& E ( \alpha X 1 + (1 − \alpha ) X 2 )\\
&=& \alpha E ( X 1 ) + (1 − \alpha ) E ( X 2 ) \\
&=& \alpha\mu + (1 − \alpha ) \mu \\ &=& \mu\\
\end{eqnarray*}

Therefore W is unbiased.
\item (ii)
MSE(W) = var(W) + {bias(W)} 2
W is unbiased
∴ 
\begin{eqnarray*}MSE(W) &=& var(W)\\
&=& var( \alpha X 1 + (1 − \alpha ) X 2 )\\
&=& \alpha 2 var( X 1 ) + (1 − \alpha ) 2 var( X 2 ) (independent samples)\\
&=& \alpha 2\\
\end{eqnarray*}
\item (iii)
\[
\sigma^2_1
\sigma^2
+ (1 − \alpha ) 2 2
n
n
\sigma^2
\sigma^2\]
\[d MSE
= 2 \alpha 1 − 2(1 − \alpha ) 2
d \alpha
n
n\]

d MSE
= 0 ⇒ ( \sigma^2_1 + \sigma^2 2 ) \alpha = \sigma^2 2
d \alpha
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\sigma^22
∴ \alpha=
\sigma^2_1 + \sigma^2 2
d 2 MSE
d \alpha 2
(iv)
= 2
\sigma^2_1
\sigma^2
+ 2 2 > 0 ∴ minimum
n
n
\item
The maximum likelihood estimator of $\mu$ in the special case with
$\sigma^2_1 = \sigma^2 2 = \sigma^2$ is
\mu ˆ =
=
nX + nX 2
sum of observations
= 1
number of observations
2 n
1
1
X 1 + X 2
2
2
This is the same as W since
\alpha=
9
\item (i)
\sigma^2 2
\sigma^2_1
+ \sigma^2 2
=
\sigma^2
2
\sigma +\sigma
2
=
1
1
1
⇒ W = X 1 + X 2 .
2
2
2
\end{itemize}
\end{document}
